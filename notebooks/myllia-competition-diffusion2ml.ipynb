{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Myllia - echoes of silenced genes\n",
    "---\n",
    "\n",
    "**authors**: [fsb2210](https://www.kaggle.com/fsb2210), [julianc93](https://www.kaggle.com/julianc93)\n",
    "\n",
    "The task is to train a model that is able to predict *expression changes in scRNA-seq data induced by CRISPRi perturbations*. For that, we have a dataset of 80 different perturbations and the *average expression values* of genes, plus an unperturbed case (*non-targeting sgRNA*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This problem esentially consists of inputs of strings given by the `pert_symbol` column and an output vector space of dimension equal to the number of columns minus the one corresponding to the `pert_symbol` (i.e., a 5127-dimensional space).\n",
    "\n",
    "Clearly, this dataset needs to be preprocessed. In particular, we are going to:\n",
    "\n",
    "- treat each of the genes in the sample as tokens and get embeddings from them using a pre-trained neural network,\n",
    "- compute delta averages for each of the output vectors,\n",
    "- train a machine learning (ML) model using the preprocessed datasets created during steps 1 and 2,\n",
    "- predict delta averages on new genes using the validation set (`pert_ids_val.csv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    "> \n",
    "> Dependencies list is: matplotlib numpy pandas tqdm scipy scikit-learn seabon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import RidgeCV, MultiTaskElasticNetCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/usr/lib/relative-wmae-multiplied-by-wcosine\")\n",
    "\n",
    "from metric import _score_impl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define global options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"\"\n",
    "\n",
    "# directory with data files\n",
    "data_dir = \"../data\"\n",
    "working_dir = \"../data\"\n",
    "\n",
    "# random state integer value for reproducibility concerns\n",
    "random_state = 42\n",
    "\n",
    "# threshold value to consider PPI interactions ONLY if score is larger than this\n",
    "score_threshold = 0.5\n",
    "\n",
    "# restart probability of continuing the perturbation in the network. if this is an array, compute every one of them and concatenate results\n",
    "alpha_RW = 0.75\n",
    "\n",
    "# variance percentile to keep genes that vary significantly across perturbations, used for training a ML model\n",
    "reduce_by_variance = False\n",
    "variance_percentile = 80  # keep top 20%\n",
    "\n",
    "# reduce dimensions of output space genes to this number in the latent space\n",
    "n_latent = 35\n",
    "\n",
    "# filename of figure\n",
    "fig_fname = f\"{experiment_name}.png\"\n",
    "\n",
    "# OOF predictions and submission filenames (DataFrame structures saved)\n",
    "oof_preds_fname = f\"{experiment_name}.csv\"\n",
    "submission_fname = f\"submissions_{experiment_name}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -o links.txt.gz https://stringdb-downloads.org/download/protein.links.v12.0/9606.protein.links.v12.0.txt.gz\n",
    "# !curl -o info.txt.gz https://stringdb-downloads.org/download/protein.info.v12.0/9606.protein.info.v12.0.txt.gz\n",
    "# ! gunzip links.txt.gz info.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links between proteins\n",
    "links_df = pd.read_csv(f\"{working_dir}/links.txt\", sep=\" \")\n",
    "# protein name -> stringID\n",
    "info_df = pd.read_csv(f\"{working_dir}/info.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & validation sets\n",
    "train_df = pd.read_csv(f\"{data_dir}/training_data_means.csv\")\n",
    "val_df = pd.read_csv(f\"{data_dir}/pert_ids_val.csv\")\n",
    "# ground truth\n",
    "ground_truth = pd.read_csv(f\"{data_dir}/training_data_ground_truth_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare gene lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_df = train_df[train_df.pert_symbol != \"non-targeting\"]\n",
    "cntrl_df = train_df[train_df.pert_symbol == \"non-targeting\"]\n",
    "\n",
    "# gene inputs (labels)\n",
    "train_pert_genes = pert_df[\"pert_symbol\"].unique().tolist()\n",
    "val_pert_genes = val_df[\"pert\"].tolist()\n",
    "all_pert_genes = train_pert_genes + val_pert_genes\n",
    "\n",
    "# gene outputs (labels)\n",
    "output_genes = [c for c in pert_df.columns if c != \"pert_symbol\"]\n",
    "\n",
    "core_genes = list(set(all_pert_genes + output_genes))\n",
    "print(f\"- total number of genes in the training + validation sets (a.k.a., core_genes): {len(core_genes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ground truth values matching the training set. These values will be used during model training, in order to find the best solution (maximizing the metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cols = [f\"w_{g}\" for g in output_genes]\n",
    "# true values (used later on, when computing metric)\n",
    "Y_true = ground_truth[output_genes].values\n",
    "W_true = ground_truth[weight_cols].values\n",
    "baseline_true = ground_truth[\"baseline_wmae\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary mapping string ids from proteins to names. E.g., \"9606.ENSP00000000233\" -> \"ARF5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_name = dict(zip(info_df[\"#string_protein_id\"], info_df[\"preferred_name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add protein names to links DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecesary columns & compute weights between [0, 1]\n",
    "edges_df = links_df.copy()\n",
    "edges_df[\"weight\"] = edges_df[\"combined_score\"] / 1000.0\n",
    "edges_df = edges_df.drop(columns=[\"combined_score\"])\n",
    "\n",
    "edges_df[\"protein1\"] = edges_df[\"protein1\"].map(string_to_name)\n",
    "edges_df[\"protein2\"] = edges_df[\"protein2\"].map(string_to_name)\n",
    "edges_df = edges_df.dropna(subset=[\"protein1\", \"protein2\"])  # remove unmapped edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just use links that have a score above a threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = edges_df[edges_df[\"weight\"] >= score_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all interactions where at least one side of the proteins is in the core list (`core_genes`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = edges.protein1.isin(core_genes) | edges.protein2.isin(core_genes)\n",
    "relevant_edges = edges[mask]\n",
    "relevant_edges.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create map between genes and index. First for the connected genes (cases with interactions) then for the ghost ones (no interactions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_genes = set(relevant_edges.protein1.unique()) | set(relevant_edges.protein2.unique())\n",
    "\n",
    "# genes with edges\n",
    "final_gene_set = connected_genes.copy()\n",
    "\n",
    "# genes missing\n",
    "missing_core = set(core_genes) - connected_genes\n",
    "\n",
    "len(connected_genes), len(missing_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upda final list of genes with connected & missing ones\n",
    "final_gene_set.update(missing_core)\n",
    "\n",
    "# mapping between gene names and indices\n",
    "gene_to_idx = {gene: idx for idx, gene in enumerate(final_gene_set)}\n",
    "\n",
    "# and we can define the number of nodes!\n",
    "n_nodes = len(gene_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"- number of connected genes: {len(connected_genes)}\")\n",
    "print(f\"- number of genes missing in STRING database: {len(missing_core)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = relevant_edges[\"protein1\"].map(gene_to_idx).values\n",
    "dst = relevant_edges[\"protein2\"].map(gene_to_idx).values\n",
    "w = relevant_edges[\"weight\"].values\n",
    "\n",
    "rows = np.concatenate([src, dst])\n",
    "cols = np.concatenate([dst, src])\n",
    "data = np.concatenate([w, w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the adjacency matrix along with the degree matrix (both being sparse matrices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted adjancency matrix\n",
    "A = sp.csr_matrix((data, (rows, cols)), shape=(n_nodes, n_nodes))\n",
    "\n",
    "# add a small self-loop to prevent a division by zero when computing D^-1.\n",
    "eye = sp.eye(n_nodes, format=\"csr\")\n",
    "A = A + eye\n",
    "\n",
    "# degree matrix\n",
    "deg = np.array(A.sum(axis=1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iso = np.sum(deg == 0)\n",
    "num_con = n_nodes - num_iso\n",
    "print(f\"- final graph nodes: {n_nodes}\")\n",
    "print(f\"- minimum degree: {deg.min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the random-walk Laplacian matrix, defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = I - A D^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "where $I$ is the identity matrix, $D^{-1}$ is the inverse of the degree matrix and $A$ is the adjacency matrix.\n",
    "\n",
    "This Laplacian matrix is the kernel of the heat equation (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse matrix of degree\n",
    "deg_inv = np.zeros_like(deg)\n",
    "mask = deg > 0\n",
    "deg_inv[mask] = 1 / deg[mask]\n",
    "D_inv = sp.diags(deg_inv)\n",
    "\n",
    "# Laplacian matrix\n",
    "## L = sp.eye(n_nodes) - A.dot(D_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion operator\n",
    "\n",
    "Now we are ready to create the operator which is needed to solve diffusion for diffferent perturbations.\n",
    "\n",
    "The operator is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    x = (I + \\epsilon L) x_0\n",
    "\\end{equation}\n",
    "\n",
    "where $x$ is the solution we seek, that is, it is the perturbation vector integrated over time (steady-state). $x_0$ is the initial perturbation vector which is a one-hot vector localized at the gene that is perturbed. This is defined for a single \"timestep\".\n",
    "\n",
    "Thus, the integrated solution is found to be:\n",
    "\n",
    "\\begin{equation}\n",
    "    x = (1 - \\alpha) (I - \\alpha  A D^{-1})^{-1} x_0\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is a control parameter for the restart of the perturbation to its initial location in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create auxiliary functions:\n",
    "\n",
    "1. to one-hot encode vectors,\n",
    "2. to solve the diffusion equation (diffusion operator) using the **conjugate gradient** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(gene, gene_to_idx):\n",
    "    x0 = np.zeros(len(gene_to_idx))\n",
    "    x0[gene_to_idx[gene]] = 1.0\n",
    "    return x0\n",
    "\n",
    "def diffuse_gene(gene, gene_to_idx, LU):\n",
    "    x0 = one_hot(gene, gene_to_idx)\n",
    "    x = LU.solve(x0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the nature of the matrix, we can compute its LU decomposition using scipy.sparse.linalg method `splu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "if isinstance(alpha_RW, list):\n",
    "    all_X_train, all_X_val = [], []\n",
    "    for alpha in alpha_RW:\n",
    "        # M operator\n",
    "        M = (1/(1 - alpha_RW)) * (sp.eye(n_nodes) - alpha_RW * A.dot(D_inv))\n",
    "        M_csc = M.tocsc()\n",
    "        # LU decomposition\n",
    "        LU = sp.linalg.splu(M_csc)\n",
    "        # diffuse on training & valid sets\n",
    "        X_alpha_train = np.vstack([diffuse_gene(g, gene_to_idx, LU) for g in train_pert_genes])\n",
    "        X_alpha_val = np.vstack([diffuse_gene(g, gene_to_idx, LU) for g in val_pert_genes])\n",
    "\n",
    "        all_X_train.append(X_alpha_train)\n",
    "        all_X_val.append(X_alpha_val)\n",
    "\n",
    "    # stack diffusions\n",
    "    X = np.hstack(all_X_train)\n",
    "    X_val = np.hstack(all_X_val)\n",
    "\n",
    "else:\n",
    "    # M operator\n",
    "    M = (1/(1 - alpha_RW)) * (sp.eye(n_nodes) - alpha_RW * A.dot(D_inv))\n",
    "    M_csc = M.tocsc()\n",
    "    # LU decomposition\n",
    "    LU = sp.linalg.splu(M_csc)\n",
    "    # stack diffusions\n",
    "    X = np.vstack([diffuse_gene(g, gene_to_idx, LU) for g in train_pert_genes])\n",
    "    X_val = np.vstack([diffuse_gene(g, gene_to_idx, LU) for g in val_pert_genes])\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"LU decomposition completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage for a single gene \"ACLY\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gene_example = \"ACLY\"\n",
    "\n",
    "x0 = one_hot(gene_example, gene_to_idx)\n",
    "idx = gene_to_idx[gene_example]\n",
    "\n",
    "x = LU.solve(x0)\n",
    "r = M @ x - x0\n",
    "res = np.linalg.norm(r)\n",
    "\n",
    "print(f\"perturb. around inits: {x[idx-5:idx+5]}\")\n",
    "print(f\"residual (r = (M @ x) - x0): {res:.4e}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our input and output (`X`, `Y`) datasets to be used for training a machine learning model.\n",
    "\n",
    "- `X` will be the full diffusion embedding matrix for each of the training perturbations,\n",
    "- `Y` is the delta expressions of the 5127 genes of the training set.\n",
    "\n",
    "Same idea for the `X_val` and `Y_val` for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensions of inputs to genes that actually have a non-negligible variance across perturbations (CHANGE 1A)\n",
    "if reduce_by_variance:\n",
    "    gene_var = np.var(X, axis=0)\n",
    "    var_threshold = np.percentile(gene_var, variance_percentile)\n",
    "    selected_mask = gene_var >= var_threshold\n",
    "    top_ind = np.where(selected_mask)[0]\n",
    "    X = X[:, top_ind]\n",
    "    X_val = X_val[:, top_ind]\n",
    "\n",
    "# create array of delta averages\n",
    "Y_raw = pert_df.iloc[:,1:].values\n",
    "Y_control = cntrl_df.iloc[:,1:].values\n",
    "Y = Y_raw - Y_control\n",
    "\n",
    "pca_resp = PCA(n_components=n_latent, random_state=random_state)\n",
    "Y = pca_resp.fit_transform(Y)\n",
    "\n",
    "X.shape, Y.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "We are now ready to train a machine learning model using the results of the diffusion of perturbations in the graph as inputs and the computed delta expressions for 5127 genes as outputs.\n",
    "\n",
    "In order to accomplish this task, we use a nested cross-validation (CV) technique containing two loops:\n",
    "\n",
    "1. an inner CV that is used to tune the best hyperparameters of the model. takes samples of 79 perturbations, splits it into different folds to test different parameters and chooses the best ones,\n",
    "2. an outer loop (**leave-one-out**, LOO) that is used to evaluate the best trained models (from the inner loop) and it generates a prediction for *every sample* in the set without that sample ever being used to train the final model that predicted it.\n",
    "\n",
    "This is required for small datasets to avoid data leakages that would give rise to a very biased score.\n",
    "\n",
    "Thus, the workflow for a single step is:\n",
    "\n",
    "1. *split*: take sample 1 out (`va`), keep samples 2-80 (`tr`),\n",
    "2. *scale*: fit scaler on samples 2-80, then transform samples 2-80 and sample 1,\n",
    "3. *inner CV (tuning)*:\n",
    "   - `RidgeCV` takes samples 2-80,\n",
    "   - splits samples 2-80 into 3 chunks,\n",
    "   - tests different alphas to see which one works best on the chunks,\n",
    "   - picks  best parameters (e.g., `alpha` = 0.5)\n",
    "4. *final train*: `RidgeCV` retrains on all of samples 2-80 using `alpha` = 0.5,\n",
    "5. *out-of-fold (OOF) predict*: `RidgeCV` from step before makes a prediction on sample 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# alpha values to try for the ElasticNet model\n",
    "alphas_enet = [0.01, 0.1, 1, 100]\n",
    "# alpha values to try in the Ridge model\n",
    "alphas_r = [0.01, 0.1, 1, 100]\n",
    "\n",
    "oof_preds = np.zeros((len(X), len(output_genes)))\n",
    "val_preds = np.zeros((len(X_val), len(output_genes)))\n",
    "scores, mapes = [], []\n",
    "pbar = tqdm(enumerate(loo.split(X), 1), desc=\"model training\", total=len(X), initial=1)\n",
    "for k, (tr, va) in pbar:\n",
    "    X_tr, X_va = X[tr], X[va]\n",
    "    Y_tr, Y_va = Y[tr], Y[va]\n",
    "    W_va = W_true[va]\n",
    "    B_va = baseline_true[va]\n",
    "\n",
    "    # scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "    X_va_scaled = scaler.transform(X_va)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # === Ridge model\n",
    "    model_r = RidgeCV(alphas=alphas_r, cv=3)\n",
    "    model_r.fit(X_tr_scaled, Y_tr)\n",
    "    Y_pred_r = model_r.predict(X_va_scaled)\n",
    "    Y_val_r = model_r.predict(X_val_scaled)\n",
    "\n",
    "    # === ElasticNet model\n",
    "    model_enet = MultiTaskElasticNetCV(l1_ratio=0.5, alphas=alphas_enet, cv=3, random_state=random_state, n_jobs=-1, max_iter=1000)\n",
    "    model_enet.fit(X_tr_scaled, Y_tr)\n",
    "    Y_pred_enet = model_enet.predict(X_va_scaled)\n",
    "    Y_val_enet = model_enet.predict(X_val_scaled)\n",
    "\n",
    "    # average predictions\n",
    "    Y_pred = (Y_pred_enet + Y_pred_r) / 2\n",
    "    Y_pred = pca_resp.inverse_transform(Y_pred)\n",
    "    Y_va = pca_resp.inverse_transform(Y_va)\n",
    "\n",
    "    # save OOF predictions\n",
    "    oof_preds[va] = Y_pred.copy()\n",
    "\n",
    "    # make predictions on validation set, final answer is average over the entire models\n",
    "    Y_val_pred = (Y_val_enet + Y_val_r) / 2\n",
    "    val_fold_preds = pca_resp.inverse_transform(Y_val_pred)\n",
    "    val_preds += val_fold_preds / len(X)\n",
    "\n",
    "    # score of OOF prediction (comparison against ground truth)\n",
    "    fold_mape = mean_absolute_percentage_error(Y_va, Y_pred)\n",
    "    fold_score = _score_impl(Y_va, Y_pred, W_va, B_va, eps=1e-12, max_log2=5.0, cos_left=0.0, cos_right=0.2)\n",
    "    scores.append(fold_score)\n",
    "    mapes.append(fold_mape)\n",
    "\n",
    "    pbar.set_postfix({\"iter\": f\"{k}/{len(X)}\", \"score\": f\"{fold_score:.4f}\", \"mape\": f\"{fold_mape:.4f}\"})\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"- OOF mean score: {np.mean(scores):.4f} (MAPE: {np.mean(mapes):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation per gene (axis=0)\n",
    "y_true = pca_resp.inverse_transform(Y)\n",
    "corrs = []\n",
    "for i in range(y_true.shape[1]):\n",
    "    # handle cases where variance is 0 to avoid NaNs\n",
    "    if np.std(y_true[:, i]) > 1e-6:\n",
    "        corr = np.corrcoef(y_true[:, i], oof_preds[:, i])[0, 1]\n",
    "        corrs.append(corr)\n",
    "corrs = np.array(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top 50 most variable genes based on ground truth variance\n",
    "gene_vars = np.var(y_true, axis=0)\n",
    "top_idx = np.argsort(gene_vars)[-100:]\n",
    "y_true_subset = y_true[:, top_idx]\n",
    "pred_subset = oof_preds[:, top_idx]\n",
    "\n",
    "fig_shape = (16, 9)\n",
    "fig = plt.figure(figsize=fig_shape)\n",
    "\n",
    "# Top row: two heatmaps with shared colorbar\n",
    "ax1 = plt.subplot2grid((20, 20), (0, 0), colspan=8, rowspan=10)\n",
    "im1 = sns.heatmap(y_true_subset, cmap=\"coolwarm\", center=0, ax=ax1, \n",
    "                  cbar=False, vmin=-2, vmax=2)\n",
    "ax1.set_title(\"Ground truth\")\n",
    "ax1.set_ylabel(\"pert. index\")\n",
    "\n",
    "ax2 = plt.subplot2grid((20, 20), (0, 10), colspan=9, rowspan=10)\n",
    "im2 = sns.heatmap(pred_subset, cmap=\"coolwarm\", center=0, ax=ax2, \n",
    "                  cbar=True, vmin=-2, vmax=2)\n",
    "ax2.set_title(\"Prediction\")\n",
    "ax2.set_ylabel(\"pert. index\")\n",
    "\n",
    "# Bottom row: correlation histogram spanning full width\n",
    "ax3 = plt.subplot2grid((20, 20), (12, 0), colspan=18, rowspan=9)\n",
    "sns.histplot(corrs, bins=50, kde=True, ax=ax3)\n",
    "ax3.axvline(x=0, color=\"black\", linestyle=\":\", lw=3)\n",
    "ax3.set_xlabel(\"Pearson correlation per gene\")\n",
    "ax3.set_ylabel(\"count of genes\")\n",
    "\n",
    "plt.savefig(fig_fname);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we also save OOF predictions in case we want to use them for an ensemble stacking or hill climbing approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds_df = pd.DataFrame(oof_preds, columns=output_genes)\n",
    "oof_preds_df.insert(0, \"pert_symbol\", train_pert_genes)\n",
    "oof_preds_df.to_csv(oof_preds_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We are now ready to save predictions on the 60 genes found in the validation dataset along with 60 more dummy predictions for the not-yet available genes in the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ids = val_df[\"pert_id\"].tolist()\n",
    "\n",
    "preds = pd.DataFrame(val_preds, columns=output_genes)\n",
    "preds.insert(0, \"pert_id\", val_ids)\n",
    "pad = pd.DataFrame(0, index=range(60), columns=output_genes)\n",
    "pad.insert(0, \"pert_id\", [f\"pert_{i}\" for i in range(61, 121)])\n",
    "\n",
    "submission = pd.concat([preds, pad], ignore_index=True)\n",
    "submission.to_csv(submission_fname, index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f info.txt links.txt"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15390998,
     "sourceId": 127588,
     "sourceType": "competition"
    },
    {
     "datasetId": 9342986,
     "sourceId": 14626551,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 293155945,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
